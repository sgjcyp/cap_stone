{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import  mean_squared_error, r2_score\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load 'src/pipeline.py'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import  mean_squared_error, r2_score\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "def price_xform(dframe,colname):\n",
    "    dframe[colname]=dframe[colname].str.replace('$','')\n",
    "    dframe[colname]=dframe[colname].str.replace(',','')\n",
    "    dframe[colname]=pd.to_numeric(dframe[colname])\n",
    "    #return(dframe)\n",
    "\n",
    "def cat_rename(dframe,colname,src,tgt):\n",
    "    dframe[colname]=dframe[colname].str.replace(src,tgt)\n",
    "    #     dframe[colname]=dframe[colname].str.replace(',','')\n",
    "    #     dframe[colname]=pd.to_numeric(dframe[colname])\n",
    "    #     return(dframe)\n",
    "\n",
    "def create_dummies(dframe,colname):\n",
    "    #     dfnew=dframe.copy()\n",
    "    dframe=pd.get_dummies(data=dframe, columns=[colname])# create dummies and drop the parent column\n",
    "    dframe.drop(columns=[dframe.columns[-1]],inplace=True) #dropped the last column from the add dummies\n",
    "    return dframe\n",
    "\n",
    "def impute_nullrows(dframe,colname):\n",
    "    #Impute Null Rows based on the specified column\n",
    "    print(dframe[colname].isnull().sum())\n",
    "    dftmp=dframe[~dframe[colname].isnull()]   # remove nulls from one column\n",
    "    print(dftmp[colname].isnull().sum())\n",
    "    return dftmp\n",
    "\n",
    "def init():\n",
    "    pd.set_option('display.max_columns', 16)\n",
    "    pd.set_option('display.max_rows', 96)\n",
    "\n",
    "def read_data():\n",
    "    print('\\nREADING INPUT DATASET .................')\n",
    "    cal = pd.read_csv('calendar.csv.gz')\n",
    "    listd = pd.read_csv('listings.csv.gz')\n",
    "    lists = pd.read_csv('listings.csv')\n",
    "    revs = pd.read_csv('reviews.csv.gz')\n",
    "    nhood = pd.read_csv('neighbourhoods.csv')\n",
    "    purelst = pd.read_csv('listings.csv.gz')\n",
    "    # print(lists.head())\n",
    "\n",
    "    print(cal.date.min(),cal.date.max())\n",
    "    print(revs.date.min(),revs.date.max())\n",
    "\n",
    "    return(listd)\n",
    "\n",
    "def process_data(listd,dropzips):\n",
    "    print('\\nPROCESSING INPUT DATASET .................')\n",
    "    # print(listd.head())\n",
    "\n",
    "    #!!! APPLY the same for the test data also\n",
    "\n",
    "    #CLEANING UP INPUT DATASET\n",
    "    listd_drop_cols = ['scrape_id','last_scraped','experiences_offered','thumbnail_url','medium_url','xl_picture_url',\\\n",
    "                   'host_name','host_location','neighbourhood_group_cleansed','square_feet',\\\n",
    "                   'maximum_nights','is_business_travel_ready']\n",
    "    # listd_numc_cols  # Numeric Columns\n",
    "    # listd_catz_cols  # Categorical Columns\n",
    "    # print('Shape of Input Dataset',len(listd.columns), len(listd_drop_cols))\n",
    "    listd.drop(listd_drop_cols,axis=1,inplace=True )\n",
    "    # print('Shape of Output Dataset'len(listd.columns), len(listd_drop_cols))\n",
    "\n",
    "    # Convert to Booleans\n",
    "    listd['host_is_superhost'] = listd.apply(lambda x:  x.host_is_superhost=='t', axis= 1)\n",
    "    listd['host_has_profile_pic'] = listd.apply(lambda x:  x.host_has_profile_pic=='t', axis= 1)\n",
    "    listd['host_identity_verified'] = listd.apply(lambda x:  x.host_identity_verified=='t', axis= 1)\n",
    "    listd['instant_bookable'] = listd.apply(lambda x:  x.instant_bookable=='t', axis= 1)\n",
    "\n",
    "    #Transform all the price columns to remove \"$\" and \",\"\n",
    "    price_xform(listd,'price')\n",
    "    price_xform(listd,'weekly_price')\n",
    "    price_xform(listd,'monthly_price')\n",
    "    price_xform(listd,'security_deposit')\n",
    "    price_xform(listd,'cleaning_fee')\n",
    "    price_xform(listd,'extra_people')\n",
    "\n",
    "    #Transform the room_type . to create a meaningful name and create dummies\n",
    "    cat_rename(listd,'room_type','Entire home/apt','full')\n",
    "    cat_rename(listd,'room_type','Private room','pvt')\n",
    "    cat_rename(listd,'room_type','Shared room','shared')\n",
    "    listd=create_dummies(listd,'room_type')\n",
    "\n",
    "    #Transform the bed_type . to create a meaningful name and create dummies\n",
    "    listd['bed_type'] = listd.apply(lambda x: x.bed_type=='Real Bed', axis=1)\n",
    "\n",
    "    #Put a count on amenities,   IF it does not work, pick the most important feature\n",
    "    listd['amentcnt'] =   listd.apply(lambda x: len(x.amenities.split(\",\")), axis=1)\n",
    "\n",
    "    #Impute the to remove the rows with no zipcode.  !!! APPLY the same for the test data also\n",
    "    listd=listd[~listd['zipcode'].isnull()]   # remove nulls from one column\n",
    "\n",
    "    #Impute the null values with meaningful data\n",
    "    listd['cleaning_fee']=listd['cleaning_fee'].fillna(0)\n",
    "    listd['review_scores_cleanliness'] = listd['review_scores_cleanliness'].fillna(listd.review_scores_cleanliness.mean())\n",
    "    listd['review_scores_location'] = listd['review_scores_location'].fillna(listd.review_scores_location.mean())\n",
    "    listd['review_scores_value'] = listd['review_scores_value'].fillna(listd.review_scores_value.mean())\n",
    "\n",
    "    listd.loc[listd['minimum_nights']<=7,'min_night_stay'] = 'short'\n",
    "    listd.loc[(listd['minimum_nights']>7) & (listd['minimum_nights']<=32),'min_night_stay'] = 'mid'\n",
    "    listd.loc[listd['minimum_nights']>32,'min_night_stay'] = 'long'\n",
    "    listd=create_dummies(listd,'min_night_stay')\n",
    "\n",
    "    #Condensing the property_type to sub categories\n",
    "    listd.loc[listd['property_type']=='Apartment', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Condominium', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Guest suite', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Townhouse', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Guesthouse', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Tiny house', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Timeshare', 'ppt_condensed'] = 'apt'\n",
    "    listd.loc[listd['property_type']=='Serviced apartment', 'ppt_condensed'] = 'aptspl'\n",
    "    listd.loc[listd['property_type']=='Bed and breakfast', 'ppt_condensed'] = 'aptspl'\n",
    "    listd.loc[listd['property_type']=='Treehouse', 'ppt_condensed'] = 'aptspl'\n",
    "    listd.loc[listd['property_type']=='Cabin', 'ppt_condensed'] = 'aptspl'\n",
    "    listd.loc[listd['property_type']=='Bus', 'ppt_condensed'] = 'auto'\n",
    "    listd.loc[listd['property_type']=='Boat', 'ppt_condensed'] = 'auto'\n",
    "    listd.loc[listd['property_type']=='Camper/RV', 'ppt_condensed'] = 'auto'\n",
    "    listd.loc[listd['property_type']=='Hostel', 'ppt_condensed'] = 'hostel'\n",
    "    listd.loc[listd['property_type']=='Boutique hotel', 'ppt_condensed'] = 'hotel'\n",
    "    listd.loc[listd['property_type']=='Hotel', 'ppt_condensed'] = 'hotel'\n",
    "    listd.loc[listd['property_type']=='Resort', 'ppt_condensed'] = 'hotel'\n",
    "    listd.loc[listd['property_type']=='Aparthotel', 'ppt_condensed'] = 'hotel'\n",
    "    listd.loc[listd['property_type']=='House', 'ppt_condensed'] = 'house'\n",
    "    listd.loc[listd['property_type']=='Bungalow', 'ppt_condensed'] = 'house'\n",
    "    listd.loc[listd['property_type']=='Cottage', 'ppt_condensed'] = 'house'\n",
    "    listd.loc[listd['property_type']=='Villa', 'ppt_condensed'] = 'house'\n",
    "    listd.loc[listd['property_type']=='Other', 'ppt_condensed'] = 'other'\n",
    "    listd.loc[listd['property_type']=='Loft', 'ppt_condensed'] = 'room'\n",
    "\n",
    "    listd=create_dummies(listd,'ppt_condensed')\n",
    "\n",
    "    if(dropzips):\n",
    "        listd.drop('zipcode',axis=1,inplace=True)\n",
    "    else:\n",
    "        listd=listd[listd.zipcode!='-- default zip code --']\n",
    "        listd=create_dummies(listd,'zipcode')\n",
    "\n",
    "    #imputing some more missing values\n",
    "    listd['cleaning_fee'].fillna(0, inplace=True)\n",
    "    listd['beds'].fillna(0, inplace=True)\n",
    "    listd['bathrooms'].fillna(0, inplace=True)\n",
    "\n",
    "    #Imputed the rows with price is equal to zero\n",
    "    listd=listd[listd.price>0]\n",
    "\n",
    "    listd_drop_cols2=['id', 'listing_url', 'name', 'summary', 'space', 'description',\\\n",
    "       'neighborhood_overview', 'notes', 'transit', 'access', 'interaction',\\\n",
    "       'house_rules', 'picture_url', 'host_id', 'host_url', 'host_since',\\\n",
    "       'host_about', 'host_response_time', 'host_response_rate',\\\n",
    "       'host_acceptance_rate', 'host_thumbnail_url',\\\n",
    "       'host_picture_url', 'host_neighbourhood', 'host_listings_count',\\\n",
    "       'host_total_listings_count', 'host_verifications',\\\n",
    "       'host_has_profile_pic', 'street',\\\n",
    "       'neighbourhood', 'neighbourhood_cleansed', 'city', 'state',\\\n",
    "       'market', 'smart_location', 'country_code', 'country', 'latitude',\\\n",
    "       'longitude', 'is_location_exact', 'property_type',\\\n",
    "       'amenities',\\\n",
    "       'weekly_price', 'monthly_price', 'security_deposit',\\\n",
    "       'extra_people', 'minimum_nights', 'calendar_updated',\\\n",
    "       'has_availability', 'availability_30', 'availability_60',\\\n",
    "       'availability_90', 'availability_365', 'calendar_last_scraped',\\\n",
    "       'number_of_reviews', 'first_review', 'last_review',\\\n",
    "       'review_scores_rating', 'review_scores_accuracy',\\\n",
    "       'review_scores_cleanliness', 'review_scores_checkin',\\\n",
    "       'review_scores_communication', 'review_scores_location',\\\n",
    "       'review_scores_value', 'requires_license', 'license',\\\n",
    "       'jurisdiction_names', 'instant_bookable', 'cancellation_policy',\\\n",
    "       'require_guest_profile_picture', 'require_guest_phone_verification',\\\n",
    "       'calculated_host_listings_count', 'reviews_per_month']\n",
    "    listd.drop(listd_drop_cols2,axis=1,inplace=True )\n",
    "\n",
    "    listd['host_is_superhost'] = (listd['host_is_superhost'] == True).astype(int)\n",
    "    listd['host_identity_verified'] = (listd['host_identity_verified'] == True).astype(int)\n",
    "    listd['bed_type'] = (listd['bed_type'] == True).astype(int)\n",
    "\n",
    "    print(listd.shape)\n",
    "    return(listd)\n",
    "\n",
    "def create_database(dfclean):\n",
    "    print('\\nCREATING A CLEAN TRAIN TEST DATASET .................')\n",
    "    print(dfclean.shape)\n",
    "    print(dfclean.head())\n",
    "\n",
    "    y= dfclean.price\n",
    "    X= dfclean.copy()\n",
    "    X=X.drop(['price'],axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=47)\n",
    "    print(X_train.shape, X_test.shape)\n",
    "    print(y_train.shape, y_test.shape)\n",
    "\n",
    "    X_train.to_pickle('pklz/rand_split/X_train.pkl')\n",
    "    y_train.to_pickle('pklz/rand_split/y_train.pkl')\n",
    "    X_test.to_pickle('pklz/rand_split/X_test.pkl')\n",
    "    y_test.to_pickle('pklz/rand_split/y_test.pkl')\n",
    "\n",
    "    dfclean_lt_500=dfclean[dfclean.price<500]\n",
    "    dfclean_gt_500=dfclean[dfclean.price>500]\n",
    "    dfclean_gt_500=dfclean_gt_500[dfclean_gt_500.price<1500]\n",
    "\n",
    "    y=dfclean_lt_500.price\n",
    "    X=dfclean_lt_500.copy()\n",
    "    X=X.drop(['price'],axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=47)\n",
    "\n",
    "    X_train.to_pickle('pklz/price_split/X_lt_train.pkl')\n",
    "    y_train.to_pickle('pklz/price_split/y_lt_train.pkl')\n",
    "    X_test.to_pickle('pklz/price_split/X_lt_test.pkl')\n",
    "    y_test.to_pickle('pklz/price_split/y_lt_test.pkl')\n",
    "\n",
    "\n",
    "    y=dfclean_gt_500.price\n",
    "    X=dfclean_gt_500.copy()\n",
    "    X=X.drop(['price'],axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=47)\n",
    "\n",
    "    X_train.to_pickle('pklz/price_split/X_gt_train.pkl')\n",
    "    y_train.to_pickle('pklz/price_split/y_gt_train.pkl')\n",
    "    X_test.to_pickle('pklz/price_split/X_gt_test.pkl')\n",
    "    y_test.to_pickle('pklz/price_split/y_gt_test.pkl')\n",
    "\n",
    "def del_regr_plot(y_act,y_pred,mdl_data):\n",
    "    #Building Residual DF\n",
    "\n",
    "    dfpred= y_act.to_frame()\n",
    "    dfpred['preds'] = y_pred\n",
    "    dfpred['resid'] = dfpred.preds-dfpred.price\n",
    "    dfpred['residpct'] = (dfpred.preds-dfpred.price)/dfpred.price*100\n",
    "    # dfpred.head()\n",
    "    resids=y_pred-y_test\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "    ax1.scatter(y_act, y_pred,  color='black')\n",
    "    ax1.set_title('act_vs_pred')\n",
    "    ax1.set_xlabel('PRICE')\n",
    "    ax1.set_ylabel('PREDICT')\n",
    "\n",
    "    ax2.scatter(y_act,resids, color='red')\n",
    "    ax2.set_title('RESIDUALS',)\n",
    "    ax2.set_xlabel('PRICE')\n",
    "    ax2.set_ylabel('RESIDUAL')\n",
    "\n",
    "    ax3.scatter(y_act,resids/y_test*100, color='blue')\n",
    "    ax3.set_title('RESIDUAL PCT')\n",
    "    ax3.set_xlabel('PRICE')\n",
    "    ax3.set_ylabel('RESIDUAL PCT')\n",
    "\n",
    "    fig.suptitle('mdl_data', fontsize=12, y=1.2)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.savefig('plots/'+'RESI_'+mdl_data+'.png', format='png')\n",
    "\n",
    "def regr_plot(y_act,y_pred,mdl_data):\n",
    "    #Building Residual DF\n",
    "\n",
    "    dfpred= y_act.to_frame()\n",
    "    dfpred['preds'] = y_pred\n",
    "    dfpred['resid'] = dfpred.preds-dfpred.price\n",
    "    dfpred['residpct'] = (dfpred.preds-dfpred.price)/dfpred.price*100\n",
    "    # dfpred.head()\n",
    "    resids=y_pred-y_test\n",
    "\n",
    "    fig = plt.figure(figsize=(12,5))\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "\n",
    "    ax1.scatter(y_pred, y_act,  color='black')\n",
    "    ax1.set_title('act_vs_pred')\n",
    "    ax1.set_xlabel('PREDICT')\n",
    "    ax1.set_ylabel('PRICE')\n",
    "\n",
    "    ax2.scatter(y_pred,resids, color='red')\n",
    "    ax2.set_title('RESIDUALS',)\n",
    "    ax2.set_xlabel('PREDICT')\n",
    "    ax2.set_ylabel('RESIDUAL')\n",
    "\n",
    "    ax3.scatter(y_pred,resids/y_test*100, color='blue')\n",
    "    ax3.set_title('RESIDUAL PCT')\n",
    "    ax3.set_xlabel('PREDICT')\n",
    "    ax3.set_ylabel('RESIDUAL PCT')\n",
    "\n",
    "    fig.suptitle('mdl_data', fontsize=12, y=1.2)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.savefig('plots/'+'RESI_'+mdl_data+'.png', format='png')\n",
    "\n",
    "def plot_feature_importance(featurelist,featureimp,name):\n",
    "    tmp_df=featurelist.to_frame(index=False)\n",
    "    tmp_df=tmp_df.rename(columns={0:'feature'})\n",
    "    tmp_df['prime']=featureimp\n",
    "    feature_df=tmp_df.sort_values(by=['prime'], ascending=True)\n",
    "    fig=plt.figure(figsize=(8,10))\n",
    "    ax1=fig.add_subplot(111)\n",
    "    ax1.barh(feature_df['feature'],feature_df['prime'],color='rgbkymc')\n",
    "    fig.suptitle(name, fontsize=12, y=1.2)\n",
    "    fig.tight_layout()\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.savefig('plots/'+'FI_'+name+'.png', format='png')\n",
    "\n",
    "def my_metrics(y_act,y_pred,mdl_data):\n",
    "    r2= (1 - (((y_act-y_pred)**2).sum()) / (((y_act-y_act.mean())**2).sum()) )\n",
    "    r2_score= (1 - ((y_act-y_pred)**2).sum() / ((y_act-y_act.mean())**2).sum() )\n",
    "    mse =  ((y_act-y_pred) ** 2).sum()/len(y_act)\n",
    "    rmse = mse ** 0.5\n",
    "    rmsle = np.sqrt(np.mean((np.log(1+y_pred) - np.log(1+y_act))**2))\n",
    "    mspct = np.abs(((y_act-y_pred)/y_act)).mean()\n",
    "\n",
    "\n",
    "    mse_prop = (np.mean(np.abs((y_act - y_pred)**2 / (y_act+1)**2  )) * 100) **0.5\n",
    "\n",
    "    #Calculate the baseline errors\n",
    "    baseline = abs(y_act.mean()-y_act)\n",
    "    #Calculate the absolute errors\n",
    "    residuals = abs(y_pred-y_test)\n",
    "    #Calculate Mean Absolute pct error\n",
    "    mape = (residuals/y_test)*100\n",
    "    #Accuracy \n",
    "    accy=100-np.mean(mape)\n",
    "    #Mean Absolute Error \n",
    "    \n",
    "    print('\\n   **************** Metrics for : ',mdl_data,' *******************')\n",
    "    print(\"   R2 (Variance Squared ERROR)                   = %.4f\"%r2)\n",
    "    print(\"   MSE (Mean Squared ERROR)                      = %.4f\"%mse)\n",
    "    print(\"   RMSE (ROOT Mean Squared ERROR)                = %.4f\"%rmse)\n",
    "    print(\"   RMSLE (ROOT Mean Squared Logrithmic ERROR)    = %.4f\"%rmsle)\n",
    "    print(\"   PCTE  (Percent Absolute Error)                = %.4f\\n\"%mspct)\n",
    "\n",
    "    print(\"   Baseline Mean Absolute Error                  =\",np.mean(baseline))\n",
    "    print(\"   Predicted Mean Absolute Error                 =\",np.mean(residuals))\n",
    "    print(\"   Accuracy  (100-MAPE mean absolute PCT ERR)    =\",round(accy,2), '%')\n",
    "\n",
    "    print('\\n   **************** -------------------------- *******************',len(y_act), mse_prop)\n",
    "    # print(len(y_act), mse_prop)\n",
    "\n",
    "    # regr_plot(y_act,y_pred,mdl_data)\n",
    "    return(r2,rmse,rmsle,mspct)\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    #EVALUATE THE RANDOM SEARCH BEST PARAMETERS\n",
    "    predictions = model.predict(test_features)\n",
    "    rfcv_score = model.score(test_features, test_labels)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    # print('Model Performance')\n",
    "    print('   R2 Score = {:0.2f}.'.format(rfcv_score))\n",
    "    print('   Average Error: {:0.4f}'.format(np.mean(errors)))\n",
    "    print('   Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    # print('   My R2 Score is : ',my_metric(test_labels,predictions))\n",
    "    return(accuracy)\n",
    "\n",
    "def estimate_tree(mdl,X_train, y_train,X_test,y_test):\n",
    "    treelist=[1,5,10,15,25,50,100,150,200,250,300,400,500,700,900,1200,1500,1800,2000]\n",
    "    for trees in treelist:\n",
    "    # for trees in range(25,300,25):\n",
    "        mdl.n_estimators = trees\n",
    "        mdl.fit(X_train, y_train)\n",
    "        print('         ',trees, mdl.score(X_test,y_test))\n",
    "\n",
    "def run_randforest(X_train,y_train,X_test,y_test,name):\n",
    "    print('\\nRUNNING RANDOM FOREST .................  :',name)\n",
    "\n",
    "    Z_train=X_train.copy()\n",
    "    Z_test=X_test.copy()\n",
    "    # X_train.drop('zipcode',axis=1,inplace=True)\n",
    "    # X_test.drop('zipcode',axis=1,inplace=True)\n",
    "\n",
    "    regr = RandomForestRegressor(random_state=0)\n",
    "    regr.fit(X_train, y_train)\n",
    "    # RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
    "    #        max_features='auto', max_leaf_nodes=None,\n",
    "    #        min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "    #        min_samples_leaf=1, min_samples_split=2,\n",
    "    #        min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
    "    #        oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "    # print(regr.predict([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n",
    "    y_predRF=regr.predict(X_test)\n",
    "    my_metrics(y_test,y_predRF,name)\n",
    "\n",
    "    featurelist=X_train.columns\n",
    "    featureimp= regr.feature_importances_\n",
    "    plot_feature_importance(featurelist,featureimp,name)\n",
    "    regr_plot(y_test,y_predRF,name)\n",
    "\n",
    "    estimate_tree(regr,X_train, y_train,X_test,y_test)\n",
    "\n",
    "    # Random Hyper Parameter Grid\n",
    "    \n",
    "    # n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)] # Number of trees in random forest    \n",
    "    n_estimators = [200] # Number of trees in random forest    \n",
    "    max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\n",
    "    max_depth.append(None)    \n",
    "    min_samples_split = [2, 5, 10] # Minimum number of samples required to split a node    \n",
    "    min_samples_leaf = [1, 2, 4] # Minimum number of samples required at each leaf node    \n",
    "    bootstrap = [True, False] # Method of selecting samples for training each tree    \n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "    # print(random_grid)\n",
    "\n",
    "    # Use the random grid to search for best hyperparameters\n",
    "        # Random search of parameters, using 3 fold cross validation, \n",
    "        # search across 100 different combinations, and use all available cores\n",
    "    rf = RandomForestRegressor() # First create the base model to tune\n",
    "    rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)\n",
    "    rf_random.fit(X_train, y_train) # Fit the random search model\n",
    "    print(rf_random.best_params_)\n",
    "    print(rf_random.best_score_)\n",
    "\n",
    "    print(\"   ********************* BASE MODEL Performance *********************\")\n",
    "    base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "    print(\"   ********************* RANDOM MODEL Performance *********************\")\n",
    "    best_random = rf_random.best_estimator_\n",
    "    random_accuracy = evaluate(best_random, X_test, y_test)\n",
    "    print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "    y_pred_BFF=best_random.predict(X_test)\n",
    "    my_metrics(y_test,y_pred_BFF,name+'BFF')\n",
    "    regr_plot(y_test,y_pred_BFF,name+'BFF')\n",
    "\n",
    "    # Z_test=X_test.copy()\n",
    "    Z_test['act']=y_test\n",
    "    Z_test['pred']=y_pred_BFF\n",
    "    Z_test['resid']=Z_test['pred']-Z_test['act']\n",
    "    # print(Z_test.head().T)\n",
    "    return(Z_test)\n",
    "\n",
    "def run_gradientboost(X_train,y_train,X_test,y_test,name):\n",
    "    print('\\nRUNNING GRADIENT BOOST REGRESSION .................  :',name)\n",
    "\n",
    "    Z_train=X_train.copy()\n",
    "    Z_test=X_test.copy()\n",
    "    # X_train.drop('zipcode',axis=1,inplace=True)\n",
    "    # X_test.drop('zipcode',axis=1,inplace=True)\n",
    "\n",
    "    params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\\\n",
    "          'learning_rate': 0.01, 'loss': 'ls', 'random_state':0}\n",
    "    params = {'random_state':0}\n",
    "\n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    # gbr = GradientBoostingRegressor(max_depth=2, random_state=0)\n",
    "    gbr.fit(X_train, y_train)\n",
    "\n",
    "    y_predGBR = gbr.predict(X_test)\n",
    "    mse= mean_squared_error(y_test,y_predGBR)\n",
    "    print(\"MSE: %.4f\" %mse)\n",
    "    # print(\"My GBR Score : %.4f\",my_metric(y_test,y_predGBR))\n",
    "\n",
    "    # print(gbr.predict([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))\n",
    "    y_pred=gbr.predict(X_test)\n",
    "    my_metrics(y_test,y_pred,name)\n",
    "\n",
    "    featurelist=X_train.columns\n",
    "    featureimp= gbr.feature_importances_\n",
    "    plot_feature_importance(featurelist,featureimp,name)\n",
    "    regr_plot(y_test,y_pred,name)\n",
    "\n",
    "    estimate_tree(gbr,X_train, y_train,X_test,y_test)\n",
    "\n",
    "    # Gradient Hyper Parameter Grid - Random\n",
    "\n",
    "    # Number of trees in random forest\n",
    "    # n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "    n_estimators = [1000] # Number of trees in random forest\n",
    "    max_features = ['auto', 'sqrt'] # Number of features to consider at every split\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)] # Maximum number of levels in tree\n",
    "    max_depth.append(None) # Minimum number of samples required to split a node\n",
    "    min_samples_split = [2, 5, 10] # Minimum number of samples required at each leaf node\n",
    "    min_samples_leaf = [1, 2, 4] # Method of selecting samples for training each tree    \n",
    "    alpha = [float(x) for x in np.linspace(start = 0.01, stop = 0.1, num = 5)] # Learning Rate\n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "                'max_features': max_features,\n",
    "                'max_depth': max_depth,\n",
    "                'min_samples_split': min_samples_split,\n",
    "                'min_samples_leaf': min_samples_leaf,\n",
    "                'learning_rate': alpha}\n",
    "    # print(random_grid)\n",
    "\n",
    "    # Random search of parameters, using 3 fold cross validation, \n",
    "    # search across 100 different combinations, and use all available cores\n",
    "    gbreg = GradientBoostingRegressor()\n",
    "    gbreg_random = RandomizedSearchCV(estimator = gbreg, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=1, random_state=42, n_jobs = -1)\n",
    "    gbreg_random.fit(X_train, y_train)\n",
    "    print(gbreg_random.best_params_)\n",
    "    print(gbreg_random.best_score_)\n",
    "\n",
    "    print(\"   ********************* BASE MODEL Performance *********************\")\n",
    "    base_model = GradientBoostingRegressor(**params)\n",
    "    base_model.fit(X_train, y_train)\n",
    "    base_accuracy = evaluate(base_model, X_test, y_test)\n",
    "\n",
    "    print(\"   ********************* RANDOM MODEL Performance *********************\")\n",
    "    best_random = gbreg_random.best_estimator_\n",
    "    random_accuracy = evaluate(best_random, X_test, y_test)\n",
    "    print('   Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "    y_pred_BFF=best_random.predict(X_test)\n",
    "    my_metrics(y_test,y_pred_BFF,name+'BFF')\n",
    "    regr_plot(y_test,y_pred_BFF,name+'BFF')\n",
    "\n",
    "    # Z_test=X_test.copy()\n",
    "    Z_test['act']=y_test\n",
    "    Z_test['pred']=y_pred_BFF\n",
    "    Z_test['resid']=Z_test['pred']-Z_test['act']\n",
    "    # print(Z_test.head().T)\n",
    "    return(Z_test)\n",
    "\n",
    "def run_linreg(X_train,y_train,X_test,y_test,name):\n",
    "    print('\\nRUNNING LINEAR REGRESSION .................  : ',name)   \n",
    "    X_train=sm.add_constant(X_train,has_constant='add')\n",
    "    X_test=sm.add_constant(X_test,has_constant='add')\n",
    "\n",
    "    Z_train=X_train.copy()\n",
    "    Z_test=X_test.copy()\n",
    "    # X_train.drop('zipcode',axis=1,inplace=True)\n",
    "    # X_test.drop('zipcode',axis=1,inplace=True)\n",
    "\n",
    "    # print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "    # print(X_train.head().T)\n",
    "    # print(X_test.head().T)\n",
    "\n",
    "    # Statmodel Linreg\n",
    "    modelOLS = sm.OLS(y_train, X_train)\n",
    "    resultsOLS = modelOLS.fit()\n",
    "    print(resultsOLS.summary())\n",
    "    y_predOLS = resultsOLS.predict(X_test)\n",
    "    my_metrics(y_test,y_predOLS,name)\n",
    "    regr_plot(y_test,y_predOLS,name)\n",
    "\n",
    "    # Z_test=X_test.copy()\n",
    "    Z_test['act']=y_test\n",
    "    Z_test['pred']=y_predOLS\n",
    "    Z_test['resid']=Z_test['pred']-Z_test['act']\n",
    "    # print(Z_test.head().T)\n",
    "    return(Z_test)\n",
    "\n",
    "\n",
    "def pred_feature(pred_list,fname):\n",
    "    # for df_pred in pred_list:\n",
    "    #     print('analyzing --->',df_pred.name)\n",
    "    #     print(df_pred.head().T)\n",
    "\n",
    "    # fname='bedrooms'\n",
    "    funique=[]\n",
    "    dfnamelist=[fname]\n",
    "    # pred_list=[Z_test,Z_test1]\n",
    "    for dfx in pred_list:   #each predicted df\n",
    "        dfnamelist.append(dfx.name)\n",
    "        fcurrlst=dfx[fname].value_counts().index\n",
    "        for fcurr in fcurrlst:   #creating a list of unique indexes\n",
    "            if fcurr not in funique:\n",
    "                funique.append(fcurr)\n",
    "            \n",
    "    print(funique)\n",
    "    print(sorted(funique))\n",
    "    # print(funique)\n",
    "\n",
    "    # eachrow=[]\n",
    "    plotdf=pd.DataFrame(columns=[dfnamelist])\n",
    "    fcntdf=pd.DataFrame(columns=[dfnamelist])\n",
    "\n",
    "\n",
    "    for fc in sorted(funique):\n",
    "        eachrow=[]\n",
    "        fcntrow=[]\n",
    "        eachrow.append(fc)\n",
    "        fcntrow.append(fc)\n",
    "        for dfx in pred_list:\n",
    "            filtered=dfx[dfx[fname]==fc]\n",
    "            eachrow.append(np.abs(((filtered.act-filtered.pred)/filtered.act)).mean())\n",
    "            fcntrow.append((dfx[fname]==fc).sum())\n",
    "            \n",
    "    # (Z_test.bedrooms==0).sum()        \n",
    "    #     print(eachrow)\n",
    "    #     print(plotdf.shape())\n",
    "    #     plotdf=plotdf.append(pd.Series(eachrow,index=[dfnamelist]),ignore_index=True)\n",
    "        plotdf.loc[len(plotdf)] = eachrow\n",
    "        fcntdf.loc[len(fcntdf)] = fcntrow\n",
    "\n",
    "    # plotdf.linreg_times = plotdf.linreg_times*0.9\n",
    "    print(plotdf.head())\n",
    "    # dfj.append(pd.DataFrame(listj, columns=['col1','col2']),ignore_index=True)\n",
    "    dfhead=dfnamelist[1:]\n",
    "\n",
    "\n",
    "    fig2 = plt.figure(figsize=(18,10))\n",
    "    ax1 = fig2.add_subplot(1,2,1)\n",
    "    ax1 = fcntdf[dfhead].plot(kind='barh', title =\"COUNT\",figsize=(5,10),  legend=True, fontsize=9)\n",
    "    ax1.set_xlabel(fname, fontsize=9)\n",
    "    ax1.set_ylabel(\"COUNT\", fontsize=9)\n",
    "    fig2 = plt.gcf()\n",
    "    fig2.savefig('junk/'+'COUNT_'+fname+'.png', format='png')\n",
    "\n",
    "    ax2 = fig2.add_subplot(1,2,2)\n",
    "    ax2 = plotdf[dfhead].plot(kind='barh', title =\"PCT-Error\", figsize=(5,10),  legend=True, fontsize=9)\n",
    "    ax2.set_xlabel(fname, fontsize=9)\n",
    "    ax2.set_ylabel(\"PCTE\", fontsize=9)\n",
    "    fig2 = plt.gcf()\n",
    "    fig2.savefig('junk/'+'PRED_'+fname+'.png', format='png')\n",
    "\n",
    "def pred_analysis(pred_list,mdl_data):\n",
    "\n",
    "    featurelist=['bedrooms','accommodates','bathrooms','beds','bed_type']\n",
    "\n",
    "    for fname in featurelist:\n",
    "        pred_feature(pred_list,fname)\n",
    "\n",
    "    # pred_feature(pred_list,'bedrooms')\n",
    "    # pred_feature(pred_list,'accommodates')\n",
    "\n",
    "def create_time_database():\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    init()\n",
    "\n",
    "    # df = read_data()\n",
    "    # df = process_data(df,0)\n",
    "    # create_database(df)\n",
    "\n",
    "    X_train= pd.read_pickle('pklz/price_split/X_lt_train.pkl')\n",
    "    y_train= pd.read_pickle('pklz/price_split/y_lt_train.pkl')\n",
    "    X_test= pd.read_pickle('pklz/price_split/X_lt_test.pkl')\n",
    "    y_test= pd.read_pickle('pklz/price_split/y_lt_test.pkl') \n",
    "\n",
    "\n",
    "    print('MODELLING PRICE SPLIT DATA !!!!!!!!!!!!!')\n",
    "    Z_test_LIN_lt5c = run_linreg(X_train,y_train,X_test,y_test,'Lin_lt5C')\n",
    "#     Z_test_RF_lt5c  = run_randforest(X_train,y_train,X_test,y_test,'RF_lt5C')\n",
    "#     Z_test_GB_lt5c  = run_gradientboost(X_train,y_train,X_test,y_test,'GB_lt5C')\n",
    "\n",
    "    Z_test_LIN_lt5c.name = 'Z_test_LIN_lt5c'\n",
    "#     Z_test_RF_lt5c.name = 'Z_test_RF_lt5c'\n",
    "#     Z_test_GB_lt5c.name = 'Z_test_GB_lt5c'\n",
    "\n",
    "    # Running TimeSeries\n",
    "    trainlist = ['listings_20180304.csv.gz','listings_20180406.csv.gz','listings_20180509.csv.gz']\n",
    "    testlist = ['listings_20180705.csv.gz']\n",
    "    df_train = pd.concat([pd.read_csv(fcsv) for fcsv in trainlist],ignore_index=True)\n",
    "    df_test = pd.concat([pd.read_csv(fcsv) for fcsv in testlist],ignore_index=True)\n",
    "    # print('Shape of data after reading :',df_train.shape,df_test.shape)\n",
    "    df_train=process_data(df_train,1)\n",
    "    df_test=process_data(df_test,1)\n",
    "    # print('Shape of data after cleaning :',df_train.shape,df_test.shape)\n",
    "    # print('HEAD of data after cleaning :',df_train.head().T,df_test.head().T)\n",
    "    # print('Colums of data after cleaning :',df_train.columns,df_test.columns)\n",
    "\n",
    "    df_train=df_train[df_train.price<500]\n",
    "    df_test=df_test[df_test.price<500]\n",
    "\n",
    "\n",
    "    y_train= df_train.price\n",
    "    X_train= df_train.copy()\n",
    "    X_train=X_train.drop(['price'],axis=1)\n",
    "    y_test= df_test.price\n",
    "    X_test= df_test.copy()\n",
    "    X_test=X_test.drop(['price'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    # create_time_database(df_train,df_test)\n",
    "    # X_train= pd.read_pickle('pklz/times_split/X_train.pkl')\n",
    "    # y_train= pd.read_pickle('pklz/times_split/y_train.pkl')\n",
    "    # X_test= pd.read_pickle('pklz/times_split/X_test.pkl')\n",
    "    # y_test= pd.read_pickle('pklz/times_split/y_test.pkl') \n",
    "    \n",
    "    print('MODELLING TIME SPLIT DATA !!!!!!!!!!!!!')\n",
    "    Z_test_LIN_t07 = run_linreg(X_train,y_train,X_test,y_test,'Lin_t07')\n",
    "#     Z_test_RF_t07 = run_randforest(X_train,y_train,X_test,y_test,'RF_t07')\n",
    "#     Z_test_GB_t07 = run_gradientboost(X_train,y_train,X_test,y_test,'GB_t07')\n",
    "\n",
    "    Z_test_LIN_t07.name='Z_test_LIN_t07'\n",
    "#     Z_test_RF_t07.name='Z_test_RF_t07'\n",
    "#     Z_test_GB_t07.name='Z_test_GB_t07'\n",
    "    # print(Z_test_LIN_lt5c.head().T)\n",
    "\n",
    "    print('ANALYZING PREDICTIONS !!!!!!!!!!!!!')\n",
    "    pred_analysis([Z_test_LIN_lt5c,Z_test_LIN_t07],'linonly')\n",
    "    pred_analysis([Z_test_LIN_lt5c,Z_test_RF_lt5c,Z_test_GB_lt5c,Z_test_LIN_t07,Z_test_RF_t07,Z_test_GB_t07],'allmdls')\n",
    "    print('MODELLING PROCESS COMPLETE !!!!!!!!!!!!!!!!!!!!!!! ')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
